"""
Module with utility functions for working with and preprocessing
source code.
Explanation:
## Guide of preprocess_code
Most of our codes for preprecessing are in this file `preprocess_code.py`
### The usage of clang
In our project, we need to get ast tree for every C/C++ source code. This package `clang.cindex` provides us a way to generate `ast_root` from which we can walk all the children nodes in this ast tree.
### How to use the tree
When we get the tree, we want to number each tree in order to get an edgelist or something.
But every time we run `get_children` for our nodes, it returns a new object which will lose information. So you can use `concretise_ast` to solve this problem.
After that, we can number the node by using `number_ast_nodes`(start with `counter = 1`). It is easy to generate edgelist by using `generate_edgelist` funtion which walk the graph and generate the edgelist from `ast_root` generated by `clang` before.
### Graph2vec
### Dask
Since the data size is large, we use dask in our function named `preprocess_all_for_graph2vec` to do it seperately.
## Node2vec
Node2vec requires edgelist for each ast. So by simply removing the features we get node2vec function named `process_for_node2vec` which can be used in `preprocess_all_for_adjmatrix` to get adj matrix and `preprocess_all_for_node2vec`.
### Edgelist
The outputs of all the edgelists are saved into other git repo named [`uob-summer-project-node2vec`](https://github.com/xihajun/uob-summer-project-node2vec)
## Adjacacency and Feature Matrices
The adjacency matrix is saved into the file `../data/adj.pickle`.
Our adjencency matrices for some reason is undirected, to fix this issue, we just need to get rid of the up-triangle values.
The feature matrix is saved into the file `../data/feature_matrix.pickle`.
Our feature matrices have columns that represent properties of a node and each node is one-hot encoded (with each row representing a node).
"""

# import clang.cindex
import gc
import json
import os
import subprocess
# import swifter
import tempfile
import sys
import traceback

# import dask.dataframe as dd
import numpy as np
import pandas as pd

from tqdm import tqdm
import networkx as nx
import logging
from pathlib import Path
import re


def find_primary_source_file(datapoints):
    """
    Given a list of datapoints representing the files for a single
    testcase, try to find which of the files is the "primary"
    file.
    According to the Juliet documentation, this should be the
    only file which defines the main function.
    In contrast, there is only ever one piece of code in the
    vdisc dataset.
    """

    if len(datapoints) == 1:
        # VDISC case and some of Juliet
        return datapoints.iloc[0]

    elif len(datapoints) > 1:
        # Juliet only case
        for datapoint in datapoints.itertuples():
            for line in datapoint.code.split("\n"):
                if line.startswith("int main("):
                    #primary = datapoint
                    return datapoint

        return datapoints.iloc[0]


def get_info(line, info_start, info_end, separator_index_start = 0,separator_index_end = -1):
    start = line.find(info_start,separator_index_start,separator_index_end)
    end = line.rfind(info_end,separator_index_start,separator_index_end)
    info = line[start+len(info_start):end]
    info_clean_start = info.find("(")
    info_clean_end = info.rfind(")")
    if info_clean_start>-1 and info_clean_end>-1:
        info_clean = info[info_clean_start+1:info_clean_end]
    else:
        info_clean = " "
    return info_clean


def split_trees(tree):
    """
    joern 生成的 AST、PDG 等融合在一块，这个 function 把它们分开。
    """
    res = {}
    tree_type = ""
    tree_body = ""
    for line in tree.splitlines():
        if line.strip() == "":
            continue
        if line[0] == '#':
            if tree_type != "" and tree_body != "":
                res[ tree_type ] = tree_body
                tree_body = ""

            tree_type = line[2:].strip()
            # print(tree_type)
        elif tree_type != "":
            tree_body += line + "\n"
    if tree_body != "":
        res[tree_type] = tree_body
    return res

def get_joern_id(line):
    p1 = re.compile(r'joern_id_[(](.*?)[)]', re.S)
    res = re.findall(p1, line)
    if len(res) > 0:
        return res[0]
    return ''

def get_joern_type(line):
    p1 = re.compile(r'joern_type_[(](.*?)[)]', re.S)
    res = re.findall(p1, line)
    if len(res) > 0:
        return res[0]
    return ''

def get_joern_name(line):
    p1 = re.compile(r'joern_name_[(](.*?)[)]', re.S)
    res = re.findall(p1, line)
    if len(res) > 0:
        return res[0]
    return get_joern_type(line)

def get_joern_line_no(line):
    p1 = re.compile(r'joern_line_[(](.*?)[)]', re.S)
    res = re.findall(p1, line)
    if len(res) > 0:
        return res[0]
    return ''


def remove_edge_self(edges):
    res = []
    for a, b in edges:
        if a!=b:
            res.append( [a,b] )
    return res


def get_edgelist_and_feature(string_graph, code, graphtype):
    trees = split_trees(string_graph)
    if graphtype == 'PDT':
        graph_now = trees['CFG']

        edgelist = []
        feature_json = {}
        code = code.split("\n")
        graph_now = graph_now.split("\n")
        for line in graph_now:
            line = line.strip()
            separator_index = line.find(' -->> "joern_id_')
            if separator_index>0:
                try:
                    source_line = get_info(line,"joern_line_","\"",0,separator_index)
                    sink_line = get_info(line,"joern_line_","\"",separator_index_start = separator_index,separator_index_end = len(line))
                    if source_line!=sink_line:
                        edgelist.append([int(source_line),int(sink_line)])
                        feature_json[int(source_line)] = [code[int(source_line)-1].strip()]
                        if not (int(sink_line) in feature_json.keys()):
                            feature_json[int(sink_line)] = [code[int(sink_line)-1].strip()]
                except Exception as e:
                    traceback.print_exc(file=sys.stdout)
                    print("reason", e)
                    return None, None
        edgelist = remove_edge_self(edgelist)
        if len(edgelist) == 0:
            # print("  len(edgelist) == 0")
            return None, None

        # generate PDT from CFG
        reversed_cfg_edges = []
        for row in edgelist:
            reversed_cfg_edges.append([row[1], row[0]])

        G = nx.DiGraph()
        G.add_edges_from(reversed_cfg_edges)
        start = reversed_cfg_edges[0][0]
        for edge in reversed_cfg_edges:
            for node in edge:
                if node >= start:
                    start = node
        edgelist = sorted(nx.immediate_dominators(G, start).items())
        edgelist = [list([x[1], x[0]]) for x in edgelist]

        return edgelist, feature_json

    return None, None


def process_joern_for_graph2vec2(tree, contents, graph_type, graph_type_next=" ", PDT=False, Para_link=False, **kwargs):
    """
    Takes in a list of files/datapoints from juliet.csv.zip or
    vdisc_*.csv.gz (as loaded with pandas) matching one particular
    testcase, and preprocesses it ready for the baseline model.
    """

    edgelist, feature_json = get_edgelist_and_feature(tree, graph_type, contents, graph_type_next, PDT, Para_link)
    if feature_json is None:
        return None

    # graph2vec_representation = {
    #     "edges": edgelist,
    #     "features": feature_json
    # }

    # return json.dumps(graph2vec_representation)

    return edgelist, feature_json


def run_graph2vec(input_dir, output_location, num_graph2vec_workers=1,num_epoch=1):
    print("Runs graph2vec on each of the above datapoints")
    logging.info("Runs graph2vec on each of the above datapoints")
    subprocess.run([
        "python",
        "./g2v/graph2vec.py",
        "--workers",
        str(num_graph2vec_workers),
        "--epochs",
        str(num_epoch),
        "--input-path",
        input_dir,
        "--output-path",
        output_location,
    ])
    print("`-> Done.")
    logging.info("Runs graph2vec on each of the above datapoints done")

